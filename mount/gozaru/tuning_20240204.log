llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /root/mount/models/ELYZA-japanese-Llama-2-7b-fast-instruct-q2_K.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = ELYZA-japanese-Llama-2-7b-fast-instruct
llama_model_loader: - kv   2:      general.source.huggingface.repository str              = elyza/ELYZA-japanese-Llama-2-7b-fast-...
llama_model_loader: - kv   3:                   llama.tensor_data_layout str              = Meta AI original pth
llama_model_loader: - kv   4:                       llama.context_length u32              = 4096
llama_model_loader: - kv   5:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   6:                          llama.block_count u32              = 32
llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   8:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,45043]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,45043]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,45043]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - kv  20:                          general.file_type u32              = 10
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q2_K:   65 tensors
llama_model_loader: - type q3_K:  160 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: mismatch in special tokens definition ( 304/45043 vs 264/45043 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 45043
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 6.85 B
llm_load_print_meta: model size       = 2.69 GiB (3.37 BPW) 
llm_load_print_meta: general.name     = ELYZA-japanese-Llama-2-7b-fast-instruct
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.11 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/33 layers to GPU
llm_load_tensors:        CPU buffer size =  2752.83 MiB
..............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:        CPU input buffer size   =     9.01 MiB
llama_new_context_with_model:        CPU compute buffer size =   105.57 MiB
llama_new_context_with_model: graph splits (measure): 1
main: seed: 1707028864
main: model base = '/root/mount/models/ELYZA-japanese-Llama-2-7b-fast-instruct-q2_K.gguf'
main: init model
print_params: n_vocab               : 45043
print_params: n_ctx                 : 64
print_params: n_embd                : 4096
print_params: n_ff                  : 11008
print_params: n_head                : 32
print_params: n_head_kv             : 32
print_params: n_layer               : 32
print_params: norm_rms_eps          : 0.000001
print_params: rope_freq_base        : 10000.000000
print_params: rope_freq_scale       : 1.000000
print_lora_params: n_rank_attention_norm : 1
print_lora_params: n_rank_wq             : 4
print_lora_params: n_rank_wk             : 4
print_lora_params: n_rank_wv             : 4
print_lora_params: n_rank_wo             : 4
print_lora_params: n_rank_ffn_norm       : 1
print_lora_params: n_rank_w1             : 4
print_lora_params: n_rank_w2             : 4
print_lora_params: n_rank_w3             : 4
print_lora_params: n_rank_tok_embeddings : 4
print_lora_params: n_rank_norm           : 1
print_lora_params: n_rank_output         : 4
main: total train_iterations 10
main: seen train_samples     50
main: seen train_tokens      1600
main: completed train_epochs 0
main: lora_size = 85698592 bytes (81.7 MB)
main: opt_size  = 127845136 bytes (121.9 MB)
main: opt iter 10
main: input_size = 57656352 bytes (55.0 MB)
main: compute_size = 6125272908 bytes (5841.5 MB)
main: evaluation order = RIGHT_TO_LEFT
main: tokenize training data from /root/mount/gozaru/input.txt
main: sample-start: [INST]
main: include-sample-start: false
tokenize_file: warning: found 782 samples (min length 19) that are shorter than context length of 64.
tokenize_file: total number of samples: 782
main: number of training tokens: 25462
main: number of unique tokens: 2189
main: begin training
main: work_size = 1081408 bytes (1.0 MB)
train_opt_callback: iter=    10 sample=51/782 sched=0.100000 loss=0.000000 |->
train_opt_callback: iter=    11 sample=56/782 sched=0.110000 loss=9.369513 dt=00:11:23 eta=11:11:41 |->
train_opt_callback: iter=    12 sample=61/782 sched=0.120000 loss=9.177703 dt=00:11:12 eta=10:49:43 |--->
train_opt_callback: iter=    13 sample=66/782 sched=0.130000 loss=8.485553 dt=00:10:57 eta=10:24:42 |---------->
train_opt_callback: iter=    14 sample=71/782 sched=0.140000 loss=8.125135 dt=00:10:57 eta=10:13:33 |------------->
train_opt_callback: iter=    15 sample=76/782 sched=0.150000 loss=7.707488 dt=00:10:52 eta=09:58:06 |------------------>
train_opt_callback: iter=    16 sample=81/782 sched=0.160000 loss=6.382584 dt=00:10:06 eta=09:05:24 |------------------------------->
train_opt_callback: iter=    17 sample=86/782 sched=0.170000 loss=4.793413 dt=00:09:54 eta=08:45:10 |----------------------------------------------->
train_opt_callback: iter=    18 sample=91/782 sched=0.180000 loss=3.963250 dt=00:09:51 eta=08:32:26 |------------------------------------------------------->
train_opt_callback: iter=    19 sample=96/782 sched=0.190000 loss=3.100948 dt=00:09:48 eta=08:20:08 |---------------------------------------------------------------->
save_checkpoint_lora_file: saving to /root/mount/gozaru/output/lora-20.gguf
save_checkpoint_lora_file: saving to /root/mount/gozaru/output/lora-LATEST.gguf
save_as_llama_lora: saving to /root/mount/gozaru/loraout/lora-20.bin
save_as_llama_lora: saving to /root/mount/gozaru/loraout/lora-LATEST.bin
train_opt_callback: iter=    20 sample=101/782 sched=0.200000 loss=2.416022 dt=00:09:37 eta=08:01:36 |----------------------------------------------------------------------->
train_opt_callback: iter=    21 sample=106/782 sched=0.210000 loss=2.108112 dt=00:09:35 eta=07:49:44 |-------------------------------------------------------------------------->
train_opt_callback: iter=    22 sample=111/782 sched=0.220000 loss=1.881684 dt=00:09:37 eta=07:42:15 |---------------------------------------------------------------------------->
train_opt_callback: iter=    23 sample=116/782 sched=0.230000 loss=1.825542 dt=00:09:35 eta=07:30:50 |---------------------------------------------------------------------------->
train_opt_callback: iter=    24 sample=121/782 sched=0.240000 loss=2.071372 dt=00:09:33 eta=07:19:32 |-------------------------------------------------------------------------->
train_opt_callback: iter=    25 sample=126/782 sched=0.250000 loss=1.671550 dt=00:09:38 eta=07:13:32 |------------------------------------------------------------------------------>
train_opt_callback: iter=    26 sample=131/782 sched=0.260000 loss=1.707638 dt=00:09:31 eta=06:59:11 |------------------------------------------------------------------------------>
train_opt_callback: iter=    27 sample=136/782 sched=0.270000 loss=1.504723 dt=00:09:33 eta=06:51:17 |-------------------------------------------------------------------------------->
train_opt_callback: iter=    28 sample=141/782 sched=0.280000 loss=1.548764 dt=00:09:29 eta=06:38:59 |------------------------------------------------------------------------------->
train_opt_callback: iter=    29 sample=146/782 sched=0.290000 loss=1.524502 dt=00:09:28 eta=06:28:26 |------------------------------------------------------------------------------->
save_checkpoint_lora_file: saving to /root/mount/gozaru/output/lora-30.gguf
save_checkpoint_lora_file: saving to /root/mount/gozaru/output/lora-LATEST.gguf
save_as_llama_lora: saving to /root/mount/gozaru/loraout/lora-30.bin
save_as_llama_lora: saving to /root/mount/gozaru/loraout/lora-LATEST.bin
train_opt_callback: iter=    30 sample=151/782 sched=0.300000 loss=1.521838 dt=00:09:32 eta=06:21:38 |------------------------------------------------------------------------------->
train_opt_callback: iter=    31 sample=156/782 sched=0.310000 loss=1.565458 dt=00:09:34 eta=06:13:32 |------------------------------------------------------------------------------->
train_opt_callback: iter=    32 sample=161/782 sched=0.320000 loss=1.545358 dt=00:09:45 eta=06:10:43 |------------------------------------------------------------------------------->
train_opt_callback: iter=    33 sample=166/782 sched=0.330000 loss=1.345274 dt=00:09:57 eta=06:08:28 |--------------------------------------------------------------------------------->
train_opt_callback: iter=    34 sample=171/782 sched=0.340000 loss=1.193962 dt=00:09:59 eta=05:59:35 |----------------------------------------------------------------------------------->
train_opt_callback: iter=    35 sample=176/782 sched=0.350000 loss=1.024587 dt=00:10:03 eta=05:51:54 |------------------------------------------------------------------------------------>
train_opt_callback: iter=    36 sample=181/782 sched=0.360000 loss=1.177582 dt=00:10:07 eta=05:43:59 |----------------------------------------------------------------------------------->
train_opt_callback: iter=    37 sample=186/782 sched=0.370000 loss=1.476987 dt=00:10:06 eta=05:33:46 |-------------------------------------------------------------------------------->
train_opt_callback: iter=    38 sample=191/782 sched=0.380000 loss=1.121502 dt=00:09:43 eta=05:11:19 |----------------------------------------------------------------------------------->
train_opt_callback: iter=    39 sample=196/782 sched=0.390000 loss=1.248343 dt=00:09:36 eta=04:58:03 |---------------------------------------------------------------------------------->
save_checkpoint_lora_file: saving to /root/mount/gozaru/output/lora-40.gguf
save_checkpoint_lora_file: saving to /root/mount/gozaru/output/lora-LATEST.gguf
save_as_llama_lora: saving to /root/mount/gozaru/loraout/lora-40.bin
save_as_llama_lora: saving to /root/mount/gozaru/loraout/lora-LATEST.bin
train_opt_callback: iter=    40 sample=201/782 sched=0.400000 loss=1.109527 dt=00:09:32 eta=04:46:29 |------------------------------------------------------------------------------------>
train_opt_callback: iter=    41 sample=206/782 sched=0.410000 loss=1.264916 dt=00:09:35 eta=04:37:57 |---------------------------------------------------------------------------------->
train_opt_callback: iter=    42 sample=211/782 sched=0.420000 loss=1.232695 dt=00:09:22 eta=04:22:41 |-------------------------------